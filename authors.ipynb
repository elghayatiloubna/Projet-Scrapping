{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d76263",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3940848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_google_scholar_author_selenium(url):\n",
    "    \n",
    "   # Configure Chrome to run in headless mode\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    # Initialize a Chrome WebDriver instance\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        # Navigate to the specified URL\n",
    "        driver.get(url)\n",
    "        # Wait for elements to be present before accessing them\n",
    "        driver.implicitly_wait(10)\n",
    "        # Get the HTML content of the page\n",
    "        html_content = driver.page_source\n",
    "        return html_content\n",
    "\n",
    "    finally:\n",
    "        # Close the Chrome browser instance\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b98155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_author_profile(html_content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract basic author information\n",
    "        author_results = {\n",
    "            'name': soup.select_one(\"#gsc_prf_in\") and soup.select_one(\"#gsc_prf_in\").text.strip(),\n",
    "            'position': soup.select_one(\"#gsc_prf_inw+ .gsc_prf_il\") and soup.select_one(\"#gsc_prf_inw+ .gsc_prf_il\").text.strip(),\n",
    "            'email': soup.select_one(\"#gsc_prf_ivh\") and soup.select_one(\"#gsc_prf_ivh\").text.strip(),\n",
    "            'departments': soup.select_one(\"#gsc_prf_int\") and soup.select_one(\"#gsc_prf_int\").text.strip()\n",
    "        }\n",
    "\n",
    "        # Extract h-index, i-index, and citation information\n",
    "        h_index = {\n",
    "            'all': soup.select_one(\"tr:nth-child(2) .gsc_rsb_sc1+ .gsc_rsb_std\").text,\n",
    "            'since_2018': soup.select_one(\"tr:nth-child(2) .gsc_rsb_std+ .gsc_rsb_std\").text\n",
    "        }\n",
    "\n",
    "        i_index = {\n",
    "            'all': soup.select_one(\"tr~ tr+ tr .gsc_rsb_sc1+ .gsc_rsb_std\").text,\n",
    "            'since_2018': soup.select_one(\"tr~ tr+ tr .gsc_rsb_std+ .gsc_rsb_std\").text\n",
    "        }\n",
    "\n",
    "        citations = {\n",
    "            'all': soup.select_one(\"tr:nth-child(1) .gsc_rsb_sc1+ .gsc_rsb_std\").text,\n",
    "            'since_2018': soup.select_one(\"tr:nth-child(1) .gsc_rsb_std+ .gsc_rsb_std\").text\n",
    "        }\n",
    "\n",
    "        author_results.update({'h_index': h_index, 'i_index': i_index, 'citations': citations})\n",
    "\n",
    "        # Extract co-authors information\n",
    "        co_authors = []\n",
    "        for result in soup.select('.gsc_rsb_aa'):\n",
    "            co_authors.append({\n",
    "                'name': result.select_one('.gsc_rsb_a_desc a').text.strip(),\n",
    "                'title': result.select_one('.gsc_rsb_a_ext').text.strip(),\n",
    "                'link': f\"https://scholar.google.com{result.select_one('.gsc_rsb_a_desc a')['href']}\",\n",
    "                'email': result.select_one('.gsc_rsb_a_ext.gsc_rsb_a_ext2').text.strip(),\n",
    "                'thumbnail': f\"https://scholar.googleusercontent.com/citations?view_op=view_photo&user={re.search(r'user=(.*)&', result.select_one('.gsc_rsb_a_desc a')['href']).group(1)}\"\n",
    "            })\n",
    "\n",
    "        author_results['co_authors'] = co_authors\n",
    "\n",
    "        return author_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting author profile: {e}\")\n",
    "        return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd102e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_author_info(author_url, writer_author, writer_coauthors):\n",
    "    # Scrape HTML content using the first function\n",
    "    html_content = scrape_google_scholar_author_selenium(author_url)\n",
    "\n",
    "    # Extract author profile using the updated function\n",
    "    author_profile = get_author_profile(html_content)\n",
    "\n",
    "    # Write author information to the author CSV file\n",
    "    writer_author.writerow({\n",
    "        'Name': author_profile.get('name', ''),\n",
    "        'Position': author_profile.get('position', ''),\n",
    "        'Email': author_profile.get('email', ''),\n",
    "        'Departments': author_profile.get('departments', ''),\n",
    "        'H-Index (All)': author_profile['h_index'].get('all', ''),\n",
    "        'H-Index (Since 2018)': author_profile['h_index'].get('since_2018', ''),\n",
    "        'i-Index (All)': author_profile['i_index'].get('all', ''),\n",
    "        'i-Index (Since 2018)': author_profile['i_index'].get('since_2018', ''),\n",
    "        'Citations (All)': author_profile['citations'].get('all', ''),\n",
    "        'Citations (Since 2018)': author_profile['citations'].get('since_2018', '')\n",
    "    })\n",
    "\n",
    "    # Write co-authors information to the co-authors CSV file\n",
    "    writer_coauthors.writerow({'Main Author': author_profile.get('name', '')})\n",
    "\n",
    "    for co_author in author_profile.get('co_authors', []):\n",
    "        writer_coauthors.writerow({\n",
    "            'Main Author': author_profile.get('name', ''),\n",
    "            'Name': co_author.get('name', ''),\n",
    "            'Title': co_author.get('title', ''),\n",
    "            'Link': co_author.get('link', ''),\n",
    "            'Email': co_author.get('email', ''),\n",
    "            'Thumbnail': co_author.get('thumbnail', '')\n",
    "        })\n",
    "\n",
    "    print(f\"Author information and co-authors information saved for {author_profile.get('name', '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e926fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_articles(soup):\n",
    "    try:\n",
    "        # Initialize an empty list to store article information\n",
    "        articles = []\n",
    "\n",
    "        # Iterate over each article element in the HTML\n",
    "        for el in soup.select(\"#gsc_a_b .gsc_a_t\"):\n",
    "            # Extract article details and create a dictionary\n",
    "            article = {\n",
    "                'title': el.select_one(\".gsc_a_at\").text,  # Extract article title\n",
    "                'link': \"https://scholar.google.com\" + el.select_one(\".gsc_a_at\")['href'],  # Construct full article link\n",
    "                'authors': el.select_one(\".gsc_a_at+ .gs_gray\").text,  # Extract authors\n",
    "                'publication': el.select_one(\".gs_gray+ .gs_gray\").text  # Extract publication details\n",
    "            }\n",
    "            articles.append(article)\n",
    "\n",
    "        # Clean up articles data by removing empty values from each article dictionary\n",
    "        articles = [{k: v for k, v in article.items() if v and v != \"\"} for article in articles]\n",
    "\n",
    "        # Count the number of articles\n",
    "        num_articles = len(articles)\n",
    "\n",
    "        # Return both the list of articles and the number of articles\n",
    "        return articles, num_articles\n",
    "\n",
    "    # Handle exceptions (e.g., if the HTML structure changes or the request fails)\n",
    "    except Exception as e:\n",
    "        # Print the exception message for debugging purposes\n",
    "        print(e)\n",
    "\n",
    "        # Return an empty list and 0 in case of an error\n",
    "        return [], 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3906f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_info_from_html(url):\n",
    "    # Specify user-agent in the request headers to mimic a browser\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Make an HTTP GET request to the specified URL with headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Retrieve the HTML content from the response\n",
    "        html_content = response.text\n",
    "\n",
    "        # Use BeautifulSoup to parse the HTML code\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find the element that contains the profile information\n",
    "        profile_element = soup.find('div', {'id': 'gsc_oci_table'})\n",
    "\n",
    "        # Check if the profile element is found\n",
    "        if profile_element is not None:\n",
    "            # Extracted information will be stored in this dictionary\n",
    "            profile = {}\n",
    "\n",
    "            # List of fields to extract from the profile\n",
    "            fields_to_extract = [\"Auteurs\", \"Date de publication\", \"Revue\", \"Volume\", \"Numéro\", \"Pages\", \"Nombre total de citations\"]\n",
    "\n",
    "            # Loop through the fields to extract\n",
    "            for field in fields_to_extract:\n",
    "                # Find the field element using its class and text content\n",
    "                field_element = profile_element.find('div', {'class': 'gsc_oci_field'}, text=field)\n",
    "\n",
    "                # Check if the field element is found\n",
    "                if field_element:\n",
    "                    # Find the next element with the class 'gsc_oci_value' to get the value\n",
    "                    value_element = field_element.find_next('div', {'class': 'gsc_oci_value'})\n",
    "\n",
    "                    # Store the value in the profile dictionary, stripping any leading/trailing whitespace\n",
    "                    profile[field] = value_element.text.strip() if value_element else \"\"\n",
    "\n",
    "            return profile\n",
    "        else:\n",
    "            print(\"Profil non trouvé.\")\n",
    "            return None\n",
    "    else:\n",
    "        # Print an error message if the request was not successful\n",
    "        print(f\"Erreur lors de la récupération de la page. Code de statut : {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e486cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_articles_info_from_links(article_links):\n",
    "    # Initialize an empty list to store articles data\n",
    "    all_articles_data = []\n",
    "\n",
    "    # Iterate through each article link\n",
    "    for link in article_links:\n",
    "        # Call the extract_info_from_html function for each article link\n",
    "        article_info = extract_info_from_html(link)\n",
    "\n",
    "        if article_info:\n",
    "            # Extract only the numeric part from the \"Nombre total de citations\" field\n",
    "            citations = article_info.get('Nombre total de citations', '')\n",
    "            numeric_citations = re.search(r'\\d+', citations).group() if citations else \"\"\n",
    "\n",
    "            # Add the numeric citations back to the article_info dictionary\n",
    "            article_info['Nombre total de citations'] = numeric_citations\n",
    "\n",
    "            # Append the extracted information to the list\n",
    "            all_articles_data.append(article_info)\n",
    "    # Return the list of articles data\n",
    "    return all_articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780dd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Excel file\n",
    "excel_file_path = \"C:\\\\Users\\\\lenovo\\\\Downloads\\\\resultat.xlsx\"\n",
    "\n",
    "# Load the Excel file into a pandas DataFrame\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Display the first 20 rows of the DataFrame\n",
    "df1 = df.head(38)\n",
    "\n",
    "# Display the contents of the first 20 rows\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8012090",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df1.iterrows():\n",
    "    author_url = row['bio'] \n",
    "    # Scrape HTML content using the first function\n",
    "    html_content = scrape_google_scholar_author_selenium(author_url)\n",
    "\n",
    "    # Extract author profile using the updated function\n",
    "    author_profile = get_author_profile(html_content)\n",
    "\n",
    "    # Create a CSV file for author information\n",
    "    csv_file_path_author = 'author_data.csv'\n",
    "    with open(csv_file_path_author, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames_author = ['Name', 'Position', 'Email', 'Departments', 'H-Index (All)', 'H-Index (Since 2018)', 'i-Index (All)', 'i-Index (Since 2018)', 'Citations (All)', 'Citations (Since 2018)']\n",
    "        writer_author = csv.DictWriter(csvfile, fieldnames=fieldnames_author)\n",
    "\n",
    "        # Write the header\n",
    "        writer_author.writeheader()\n",
    "\n",
    "        # Write the author information\n",
    "        writer_author.writerow({\n",
    "            'Name': author_profile.get('name', ''),\n",
    "            'Position': author_profile.get('position', ''),\n",
    "            'Email': author_profile.get('email', ''),\n",
    "            'Departments': author_profile.get('departments', ''),\n",
    "            'H-Index (All)': author_profile['h_index'].get('all', ''),\n",
    "            'H-Index (Since 2018)': author_profile['h_index'].get('since_2018', ''),\n",
    "            'i-Index (All)': author_profile['i_index'].get('all', ''),\n",
    "            'i-Index (Since 2018)': author_profile['i_index'].get('since_2018', ''),\n",
    "            'Citations (All)': author_profile['citations'].get('all', ''),\n",
    "            'Citations (Since 2018)': author_profile['citations'].get('since_2018', '')\n",
    "        })\n",
    "\n",
    "    print(f\"Author profile saved to {csv_file_path_author}\")\n",
    "\n",
    "    # Create a CSV file for co-authors\n",
    "    csv_file_path_coauthors = 'coauthors.csv'\n",
    "    with open(csv_file_path_coauthors, 'w', newline='', encoding='utf-8') as csvfile_coauthors:\n",
    "        fieldnames_coauthors = ['Main Author', 'Name', 'Title', 'Link', 'Email', 'Thumbnail']\n",
    "        writer_coauthors = csv.DictWriter(csvfile_coauthors, fieldnames=fieldnames_coauthors)\n",
    "\n",
    "        # Write the header\n",
    "        writer_coauthors.writeheader()\n",
    "\n",
    "        # Write the main author's name as the first row\n",
    "        writer_coauthors.writerow({'Main Author': author_profile.get('name', '')})\n",
    "\n",
    "        # Write the co-authors information\n",
    "        for co_author in author_profile.get('co_authors', []):\n",
    "            writer_coauthors.writerow({\n",
    "                'Main Author': author_profile.get('name', ''),\n",
    "                'Name': co_author.get('name', ''),\n",
    "                'Title': co_author.get('title', ''),\n",
    "                'Link': co_author.get('link', ''),\n",
    "                'Email': co_author.get('email', ''),\n",
    "                'Thumbnail': co_author.get('thumbnail', '')\n",
    "            })\n",
    "\n",
    "    print(f\"Co-authors information saved to {csv_file_path_coauthors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df1.iterrows():\n",
    "    author_url = row['bio']\n",
    "   \n",
    "    # Scrape HTML content of the author's profile using the first function\n",
    "    html_content_author = scrape_google_scholar_author_selenium(author_url)\n",
    "\n",
    "    # Extract the name of the main author\n",
    "    soup_author = BeautifulSoup(html_content_author, 'html.parser')\n",
    "    main_author_name = soup_author.select_one(\"#gsc_prf_in\") and soup_author.select_one(\"#gsc_prf_in\").text.strip()\n",
    "\n",
    "    # Call the extract_articles function for the author's profile page\n",
    "    articles_info, num_articles = extract_articles(soup_author)\n",
    "\n",
    "    # If there are articles, extract additional information from each article link\n",
    "    if num_articles > 0:\n",
    "        # Extract article links\n",
    "        article_links = [article['link'] for article in articles_info]\n",
    "\n",
    "        # Call the extract_articles_info_from_links function\n",
    "        all_articles_data = extract_articles_info_from_links(article_links)\n",
    "\n",
    "        # Set the fieldnames dynamically based on the keys found in all_articles_data\n",
    "        fieldnames_articles = [\"Main Author\"] + list(set(key for article_info in all_articles_data for key in article_info.keys()))\n",
    "\n",
    "        # Save the information to a CSV file\n",
    "        csv_file_path_articles = 'articles_info.csv'\n",
    "        with open(csv_file_path_articles, 'a', newline='', encoding='utf-8') as csvfile_articles:\n",
    "            writer_articles = csv.DictWriter(csvfile_articles, fieldnames=fieldnames_articles)\n",
    "\n",
    "            # If the file is empty, write the header\n",
    "            if csvfile_articles.tell() == 0:\n",
    "                writer_articles.writeheader()\n",
    "\n",
    "            # Write the article information\n",
    "            for article_data in all_articles_data:\n",
    "                # Add the main author's name to each row\n",
    "                article_data['Main Author'] = main_author_name\n",
    "\n",
    "                # Print keys for debugging\n",
    "                print(\"Keys in article_data:\", article_data.keys())\n",
    "                print(\"Fieldnames:\", fieldnames_articles)\n",
    "\n",
    "                # Write the row to the CSV file\n",
    "                writer_articles.writerow(article_data)\n",
    "\n",
    "        print(f\"Articles information appended to {csv_file_path_articles}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
